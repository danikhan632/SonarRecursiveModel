# Quick Start: Converting Qwen to Recursive Model

## üöÄ TL;DR - Complete Workflow

```bash
# Step 1: Convert Qwen to Recursive (5 minutes)
python convert_qwen_to_recursive.py \
    --qwen_model "Qwen/Qwen2-0.5B" \
    --L_layers 4 \
    --H_cycles 3 \
    --output_path qwen_recursive_init.pt

# Step 2: Fine-tune (hours to days depending on data)
python train_qwen_recursive.py \
    --checkpoint qwen_recursive_init.pt \
    --dataset_name wikitext \
    --batch_size 4 \
    --num_epochs 3 \
    --enable_deep_supervision

# Step 3: Generate text
python inference_qwen_recursive.py \
    --checkpoint qwen_recursive_finetuned_best.pt \
    --prompt "Once upon a time"
```

---

## üìã Step-by-Step Guide

### Prerequisites

```bash
pip install torch transformers datasets tqdm
```

### Step 1: Convert Qwen Model

Convert a pretrained Qwen model to recursive architecture:

```bash
python convert_qwen_to_recursive.py \
    --qwen_model "Qwen/Qwen2-0.5B" \
    --L_layers 4 \
    --H_cycles 3 \
    --L_cycles 3 \
    --seq_len 512 \
    --output_path qwen_recursive_init.pt
```

**What this does:**
- Downloads Qwen/Qwen2-0.5B from Hugging Face
- Copies embeddings ‚Üí your model
- Copies 4 transformer layers ‚Üí your L_layers (evenly sampled)
- Copies LM head ‚Üí your model
- Saves initialized model to `qwen_recursive_init.pt`

**Output:**
```
‚úì Loaded Qwen model: hidden_size=896, num_layers=24, vocab_size=151936
‚úì Initialized recursive model
‚úì Copied embeddings
‚úì Copied 4 transformer layers from indices [0, 8, 16, 23]
‚úì Copied LM head
‚úì Saved to qwen_recursive_init.pt
```

**Available Models:**
- `Qwen/Qwen2-0.5B` (896 hidden, 24 layers) - Recommended for testing
- `Qwen/Qwen2-1.5B` (1536 hidden, 28 layers) - Medium
- `Qwen/Qwen2-7B` (3584 hidden, 32 layers) - Large (requires more VRAM)

### Step 2: Fine-tune the Model

Fine-tune on your dataset:

```bash
python train_qwen_recursive.py \
    --checkpoint qwen_recursive_init.pt \
    --dataset_name wikitext \
    --dataset_config_name wikitext-2-raw-v1 \
    --batch_size 4 \
    --lr 1e-5 \
    --num_epochs 3 \
    --seq_len 128 \
    --enable_deep_supervision \
    --deep_supervision_weight 0.5 \
    --generate_every 500
```

**Key Arguments:**
- `--checkpoint`: Model from Step 1
- `--enable_deep_supervision`: Train all recursive steps (recommended)
- `--lr 1e-5`: Low learning rate since we're fine-tuning
- `--generate_every 500`: Generate sample text every 500 steps

**Expected Output:**
```
Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [10:23<00:00, loss=2.4521, lr=1e-5]

Step 500 - Generating sample:
Prompt: The quick brown fox
Output: The quick brown fox jumped over the lazy dog and ran into the forest...

Epoch 1 Summary:
  Train Loss: 2.8234
  Val Loss: 2.7156
  Perplexity: 15.12
  ‚úì New best model saved: qwen_recursive_finetuned_best.pt
```

### Step 3: Generate Text

**Option A: Single Generation**
```bash
python inference_qwen_recursive.py \
    --checkpoint qwen_recursive_finetuned_best.pt \
    --prompt "Once upon a time in a land far away" \
    --max_new_tokens 100 \
    --temperature 0.8
```

**Option B: Interactive Mode**
```bash
python inference_qwen_recursive.py \
    --checkpoint qwen_recursive_finetuned_best.pt
```

Then enter prompts interactively:
```
Prompt: Once upon a time
Generating...
Once upon a time in a small village, there lived a young girl named Alice...

Prompt: The meaning of life is
Generating...
The meaning of life is a question that has puzzled philosophers for centuries...

Prompt: /quit
Goodbye!
```

---

## üéõÔ∏è Advanced Options

### Layer Selection Strategies

Choose which Qwen layers to copy:

```bash
# Uniform sampling (default) - evenly spaced layers
--layer_selection uniform

# First N layers - early representations
--layer_selection first

# Last N layers - deep representations
--layer_selection last

# Middle N layers - balanced
--layer_selection middle
```

**Recommendation:** Use `uniform` for best coverage of Qwen's knowledge.

### Deep Supervision

Enable supervision at every recursive step:

```bash
python train_qwen_recursive.py \
    --checkpoint qwen_recursive_init.pt \
    --enable_deep_supervision \
    --deep_supervision_weight 0.5 \
    --deep_supervision_schedule linear_decay
```

**Benefits:**
- ‚úÖ Faster convergence
- ‚úÖ Better gradient flow
- ‚úÖ Learn progressive refinement

**Trade-offs:**
- ‚ùå ~2-3x slower training
- ‚ùå ~2x more memory

### Custom Dataset

Use your own dataset:

```bash
python train_qwen_recursive.py \
    --checkpoint qwen_recursive_init.pt \
    --dataset_name your/dataset \
    --dataset_config_name config_name
```

Must be a Hugging Face dataset with a `text` field.

### Gradient Accumulation

For larger effective batch sizes on limited VRAM:

```bash
--batch_size 2 \
--grad_accum_steps 4  # Effective batch size = 2 √ó 4 = 8
```

---

## üìä Expected Performance

### Qwen2-0.5B ‚Üí Recursive (L_layers=4, H_cycles=3)

| Metric | Before Fine-tuning | After Fine-tuning (3 epochs) |
|--------|-------------------|------------------------------|
| **Val Loss** | ~3.5 | ~2.7 |
| **Perplexity** | ~33 | ~15 |
| **Text Quality** | Repetitive | Coherent |
| **Training Time** | - | ~2-3 hours (WikiText-2, 1x A100) |

### Memory Requirements

| Model | L_layers | Batch Size | Peak VRAM | Training Speed |
|-------|----------|------------|-----------|----------------|
| Qwen2-0.5B | 4 | 4 | ~12 GB | ~50 tokens/sec |
| Qwen2-0.5B | 4 | 8 | ~20 GB | ~90 tokens/sec |
| Qwen2-1.5B | 6 | 2 | ~18 GB | ~25 tokens/sec |

---

## üêõ Troubleshooting

### "Out of Memory"
```bash
# Reduce batch size
--batch_size 2

# Or reduce sequence length
--seq_len 64

# Or use gradient accumulation
--batch_size 2 --grad_accum_steps 4
```

### "Generation is incoherent"
```bash
# Fine-tune for more epochs
--num_epochs 5

# Or use lower temperature
python inference_qwen_recursive.py \
    --checkpoint model.pt \
    --temperature 0.6
```

### "Training loss not decreasing"
```bash
# Check learning rate (may be too high/low)
--lr 5e-6  # Try lower

# Enable deep supervision
--enable_deep_supervision

# Increase warmup steps
--warmup_steps 500
```

### "Can't load tokenizer"
```bash
# Specify tokenizer path explicitly
python inference_qwen_recursive.py \
    --checkpoint model.pt \
    --tokenizer_path qwen_recursive_init_tokenizer
```

---

## üìÅ File Structure After Conversion

```
TinyRecursiveModels/
‚îú‚îÄ‚îÄ qwen_recursive_init.pt              # Initial converted model
‚îú‚îÄ‚îÄ qwen_recursive_init_tokenizer/      # Tokenizer files
‚îÇ   ‚îú‚îÄ‚îÄ tokenizer.json
‚îÇ   ‚îú‚îÄ‚îÄ tokenizer_config.json
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ qwen_recursive_finetuned_best.pt    # Best model during training
‚îú‚îÄ‚îÄ checkpoint_step_1000.pt             # Intermediate checkpoints
‚îî‚îÄ‚îÄ checkpoint_step_2000.pt
```

---

## üîç Comparison: Three Approaches

| Feature | Approach 1<br>(Embedding-only) | Approach 2<br>(Full Conversion)‚≠ê | Approach 3<br>(Hybrid) |
|---------|-------------------------------|----------------------------------|----------------------|
| **Script** | `train_qwen_trm.py` | `convert_qwen_to_recursive.py`<br>+ `train_qwen_recursive.py` | `train_qwen_trm_hybrid.py` |
| **Qwen Weights** | Embeddings only | All layers | Embeddings + LM head |
| **Text Generation** | ‚ùå No | ‚úÖ Yes | ‚úÖ Yes |
| **Training Speed** | Fast | Slow | Medium |
| **Model Size** | Small | Large | Medium |
| **Best For** | Research | Production | Prototyping |

**Recommendation:** Use **Approach 2** (this guide) for best results.

---

## üéØ Next Steps

1. **Start small:** Convert Qwen2-0.5B first
2. **Test generation:** Verify it works before fine-tuning
3. **Fine-tune:** Start with 1 epoch to test pipeline
4. **Evaluate:** Check perplexity and sample generations
5. **Scale up:** Try larger models or enable deep supervision
6. **Customize:** Adjust H_cycles, L_cycles based on your task

---

## üí° Tips

### For Best Text Quality
```bash
# Use more recursive cycles during inference
# Edit checkpoint config before loading:
config['H_cycles'] = 5  # More reasoning steps
```

### For Faster Training
```bash
# Disable deep supervision
# (remove --enable_deep_supervision flag)

# Use smaller sequences
--seq_len 64
```

### For Research
```bash
# Show recursive outputs during generation
python inference_qwen_recursive.py \
    --checkpoint model.pt \
    --show_recursive_steps
```

This will print the prediction from each H_cycle step!

---

## üìö See Also

- `QWEN_TO_RECURSIVE_GUIDE.md` - Detailed technical guide
- `DEEP_SUPERVISION_GUIDE.md` - Deep supervision details
- `models/recursive_reasoning/recursive_llm.py` - Model architecture

---

**Ready to start?** Run the 3-step workflow at the top of this guide! üöÄ
