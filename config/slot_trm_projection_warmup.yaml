# Slot TRM Projection Training - Phase 1 Warmup
# Train only the slot projections and delta head with frozen backbone
# This learns to project attention outputs into semantic slots without disturbing pretrained weights

model:
  # Base model configuration
  llada_model_name: "inclusionAI/LLaDA-MoE-7B-A1B-Instruct"
  freeze_backbone: true  # Keep LLaDA frozen

  # Slot TRM refiner configuration
  refiner_type: "attn_refiner"  # Use attention-based refiner with slot decomposition
  refiner_size: "base"  # tiny, base, or large

  # Slot dimensions (will be auto-configured based on refiner_size)
  # d_model will be inferred from LLaDA hidden size (~2048)
  # base: d_ctx=1024, d_reason=512, d_refine=512, d_conf=256

  # Recursive refinement
  max_recursive_steps: 4  # Start conservative (max steps for ACT, or fixed if ACT disabled)
  use_gating: true  # Learn slot contribution gates
  delta_scale: 0.02  # Very conservative delta updates (reduced from 0.1 for stability)

  # Adaptive Computation Time (ACT) - learned halting
  use_act: false  # Enable ACT for adaptive halting (saves compute)
  act_exploration_prob: 0.1  # Exploration probability during training
  act_q_loss_weight: 0.1  # Weight for Q-learning loss

  # Chunking
  chunk_size: 16
  max_chunks: 8

  # Training mode
  sft_mode: true  # Supervised fine-tuning with teacher forcing
  enable_deep_supervision: true
  deep_supervision_weight: 0.3
  mask_probability: 0.3  # Mask 30% of reasoning tokens

  # Selective refinement
  refine_low_confidence_only: true
  min_confidence: 0.5

# Trainable parameters specification
training_params:
  # Freeze everything except these modules
  trainable_modules:
    - "refinement_head"  # Train everything in the attention-based refiner
    # This includes THREE types of parameters:
    #
    # 1. ATTENTION (cross-chunk reasoning):
    #    - refinement_head.slot_refiner.blocks.*.attn.{q,k,v,o}_proj  (~67M params)
    #
    # 2. MLP/FFN (feedforward processing):
    #    - refinement_head.slot_refiner.blocks.*.ff  (~134M params)
    #    - refinement_head.slot_refiner.delta_net  (~8M params)
    #    - refinement_head.slot_refiner.confidence_head  (~0.1M params)
    #
    # 3. SLOT PROJECTIONS (semantic decomposition):
    #    - refinement_head.slot_refiner.slot_proj.W_ctx  (context from attention)
    #    - refinement_head.slot_refiner.slot_proj.W_reason  (logical state)
    #    - refinement_head.slot_refiner.slot_proj.W_refine  (update direction)
    #    - refinement_head.slot_refiner.slot_proj.W_conf  (uncertainty)
    #    (~5M params total)

  # Optionally train layer norms within the refinement head
  train_layer_norms: true

dataset:
  name: "natural_reasoning"  # Natural reasoning dataset with extended chains
  max_length: 256  # Reduced from 512 for faster training
  train_split: "train[:10000]"  # Use only 10k samples for faster iteration
  val_split: "validation[:1000]"  # Use only 1k validation samples

  # Preprocessing
  chunk_aware: true  # Format data to align with chunk boundaries
  min_chunk_chars: 300  # Minimum characters per reasoning chunk

training:
  # Warmup phase: small learning rate, few epochs
  batch_size: 16  # Increased from 8 (if OOM, reduce back to 8)
  gradient_accumulation_steps: 4  # Increased for effective batch size = 64 (more stable)
  num_epochs: 3  # Reduced from 5 for faster iteration

  # Optimizer - Muon (recommended for training projection matrices)
  use_muon: true  # Use Muon optimizer instead of AdamW
  muon_lr: 0.005  # Reduced from 0.02 for stability (was causing spikes)
  adam_lr: 0.0001  # Reduced from 0.0003 for stability
  muon_momentum: 0.95  # Muon momentum parameter
  weight_decay: 0.01

  # Fallback to AdamW if use_muon: false
  lr: 0.0005  # 5e-4 - Only used if use_muon is false
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_epsilon: 0.00000001  # 1e-8

  # Gradient management - TIGHTENED for stability
  grad_clip: 0.5  # Reduced from 1.0 - catch spikes earlier
  max_grad_norm: 0.5  # Reduced from 1.0 - prevent gradient explosions

  # Learning rate schedule
  warmup_steps: 500  # Increased from 200 for smoother warmup
  warmup_ratio: 0.1  # Increased from 0.05 (10% of training)
  lr_scheduler: "cosine"  # cosine, linear, or constant
  min_lr: 0.0000001  # Reduced from 1e-6 to 1e-7

  # Regularization
  dropout: 0.1
  attention_dropout: 0.1

  # Early stopping
  patience: 3  # Stop if no improvement for 3 epochs
  metric: "val_loss"  # Metric to monitor
  mode: "min"  # Minimize validation loss

validation:
  eval_interval: 200  # Evaluate every N steps (reduced from 500 for faster feedback)
  eval_steps: 50  # Number of validation steps (reduced from 100)
  save_best_only: true
  save_interval: 1  # Save every N epochs

system:
  device: "cuda"
  mixed_precision: true  # Use AMP for faster training
  compile: true  # torch.compile for 20-40% speedup (requires PyTorch 2.0+)

  # Data loading
  num_workers: 4
  pin_memory: true
  prefetch_factor: 2

  # Output
  output_dir: "./outputs/slot_trm_projection_warmup"
  checkpoint_dir: "./outputs/slot_trm_projection_warmup/checkpoints"
  save_total_limit: 2  # Keep only 2 best checkpoints
  checkpoint_interval: 200  # Save checkpoint every N steps (reduced from 1000)

  # Reproducibility
  seed: 42
  deterministic: false  # Set to true for full reproducibility (slower)

logging:
  # Logging backends
  use_wandb: false
  use_tensorboard: false
  use_csv: true  # Always log to CSV

  # WandB configuration
  wandb_project: "slot-trm-projection"
  wandb_run_name: "warmup-phase1-fast"
  wandb_entity: null  # Your wandb username/team

  # Logging frequency
  log_interval: 50  # Log every N steps (reduced from 10 for less overhead)
  log_grad_norms: false  # Disabled for speed
  log_param_norms: false  # Disabled for speed
  log_slot_statistics: true  # Log slot-specific metrics

  # What to log
  log_metrics:
    - "loss"
    - "refinement_steps"
    - "chunk_confidence"
    - "slot_ctx_norm"
    - "slot_reason_norm"
    - "slot_refine_norm"
    - "slot_conf_norm"
    - "delta_norm"
    - "gate_values"  # If using gating
    - "learning_rate"
    - "grad_norm"

# Advanced options
advanced:
  # Gradient checkpointing for memory efficiency
  gradient_checkpointing: true

  # Orthogonality regularization for slots
  orthogonality_weight: 0.001  # Reduced from 0.01 for stability - penalize overlap between slot projections

  # Confidence calibration
  confidence_calibration: true
  calibration_weight: 0.01  # Reduced from 0.05 for stability

  # Debug options
  detect_anomaly: false  # Enables anomaly detection (slow)
  profile: false  # Profile training step
  debug_print_interval: 100  # Print debug info every N steps
