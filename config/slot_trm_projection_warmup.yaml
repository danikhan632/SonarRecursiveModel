# Slot TRM Projection Training - Phase 1 Warmup
# Train only the slot projections and delta head with frozen backbone
# This learns to project attention outputs into semantic slots without disturbing pretrained weights

model:
  # Base model configuration
  llada_model_name: "inclusionAI/LLaDA-MoE-7B-A1B-Instruct"
  freeze_backbone: true  # Keep LLaDA frozen

  # Slot TRM refiner configuration
  refiner_type: "slot_based"  # Use slot-based refiner instead of standard
  refiner_size: "base"  # tiny, base, or large

  # Slot dimensions (will be auto-configured based on refiner_size)
  # d_model will be inferred from LLaDA hidden size (~2048)
  # base: d_ctx=1024, d_reason=512, d_refine=512, d_conf=256

  # Recursive refinement
  max_recursive_steps: 4  # Start conservative
  use_gating: true  # Learn slot contribution gates
  delta_scale: 0.1  # Conservative delta updates

  # Chunking
  chunk_size: 16
  max_chunks: 8

  # Training mode
  sft_mode: true  # Supervised fine-tuning with teacher forcing
  enable_deep_supervision: true
  deep_supervision_weight: 0.3
  mask_probability: 0.3  # Mask 30% of reasoning tokens

  # Selective refinement
  refine_low_confidence_only: true
  min_confidence: 0.5

# Trainable parameters specification
training_params:
  # Freeze everything except these modules
  trainable_modules:
    - "refinement_head"  # Train the refinement head (existing RecursiveRefinementHead)
    # This includes:
    #   - refinement_head.chunk_encoder
    #   - refinement_head.delta_generator
    #   - refinement_head.delta_projector
    #   - refinement_head.confidence_scorer

  # Optionally train layer norms within the refinement head
  train_layer_norms: true

dataset:
  name: "natural_reasoning"  # Natural reasoning dataset with extended chains
  max_length: 512
  train_split: "train"
  val_split: "validation"

  # Preprocessing
  chunk_aware: true  # Format data to align with chunk boundaries
  min_chunk_chars: 300  # Minimum characters per reasoning chunk

training:
  # Warmup phase: small learning rate, few epochs
  batch_size: 8
  gradient_accumulation_steps: 4  # Effective batch size = 32
  num_epochs: 5

  # Optimizer - Muon (recommended for training projection matrices)
  use_muon: true  # Use Muon optimizer instead of AdamW
  muon_lr: 0.02  # Muon learning rate (for 2D weight matrices)
  adam_lr: 0.0003  # Adam learning rate (for biases, norms, embeddings)
  muon_momentum: 0.95  # Muon momentum parameter
  weight_decay: 0.01

  # Fallback to AdamW if use_muon: false
  lr: 0.0005  # 5e-4 - Only used if use_muon is false
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_epsilon: 0.00000001  # 1e-8

  # Gradient management
  grad_clip: 1.0
  max_grad_norm: 1.0

  # Learning rate schedule
  warmup_steps: 200
  warmup_ratio: 0.05  # 5% of training
  lr_scheduler: "cosine"  # cosine, linear, or constant
  min_lr: 0.000001  # 1e-6

  # Regularization
  dropout: 0.1
  attention_dropout: 0.1

  # Early stopping
  patience: 3  # Stop if no improvement for 3 epochs
  metric: "val_loss"  # Metric to monitor
  mode: "min"  # Minimize validation loss

validation:
  eval_interval: 500  # Evaluate every N steps
  eval_steps: 100  # Number of validation steps
  save_best_only: true
  save_interval: 1  # Save every N epochs

system:
  device: "cuda"
  mixed_precision: true  # Use AMP for faster training
  compile: false  # torch.compile (requires PyTorch 2.0+)

  # Data loading
  num_workers: 4
  pin_memory: true
  prefetch_factor: 2

  # Output
  output_dir: "./outputs/slot_trm_projection_warmup"
  checkpoint_dir: "./outputs/slot_trm_projection_warmup/checkpoints"
  save_total_limit: 3  # Keep only 3 best checkpoints
  checkpoint_interval: 1000  # Save checkpoint every N step

  # Reproducibility
  seed: 42
  deterministic: false  # Set to true for full reproducibility (slower)

logging:
  # Logging backends
  use_wandb: false
  use_tensorboard: false
  use_csv: true  # Always log to CSV

  # WandB configuration
  wandb_project: "slot-trm-projection"
  wandb_run_name: "warmup-phase1"
  wandb_entity: null  # Your wandb username/team

  # Logging frequency
  log_interval: 10  # Log every N steps
  log_grad_norms: true
  log_param_norms: true
  log_slot_statistics: true  # Log slot-specific metrics

  # What to log
  log_metrics:
    - "loss"
    - "refinement_steps"
    - "chunk_confidence"
    - "slot_ctx_norm"
    - "slot_reason_norm"
    - "slot_refine_norm"
    - "slot_conf_norm"
    - "delta_norm"
    - "gate_values"  # If using gating
    - "learning_rate"
    - "grad_norm"

# Advanced options
advanced:
  # Gradient checkpointing for memory efficiency
  gradient_checkpointing: true

  # Orthogonality regularization for slots
  orthogonality_weight: 0.01  # Penalize overlap between slot projections

  # Confidence calibration
  confidence_calibration: true
  calibration_weight: 0.05

  # Debug options
  detect_anomaly: false  # Enables anomaly detection (slow)
  profile: false  # Profile training step
  debug_print_interval: 100  # Print debug info every N steps
